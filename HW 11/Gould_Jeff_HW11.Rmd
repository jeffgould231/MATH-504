---
title: "Math 504 HW11"
author: "Jeff Gould"
date: "4/1/2020"
output: pdf_document
header-includes:
  - \usepackage{amssymb, amsmath, amsthm, verbatim, graphicx, courier}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(splines)
```

## 2

### a


$$S(x) = \sum_{j=1}^D\alpha_j h_j (x) = \alpha_1h_1(x) +\alpha_2h_2(x) +\dots +\alpha_Dh_D(x)$$
which create a linear function space $F$, 
We have previously shown that 
$$\min_{f\in F} \sum_{i=1}^N(y_i - f(x^{(i)}))^2 = \min_{\alpha \in \mathbb{R}^k} ||y-B\alpha||^2$$
where $B$ is an $N \times K$ matrix of functions of $b_i(x)$, where entry $B_{kl} = b_l(x_k)$ in a linear function space $F$, and $\alpha$ is the coefficients.
So with $S(x)$ being the $f(x)$, $h_j(x)$ is analogous to $b_j(x)$, and the $\alpha$'s fill the same roll. Then 

$$\min_{S\in F}\sum_{i=1}^N (y_i - S(x_i))^2 = \min_{\alpha \in \mathbb{R}^k}||y - B \alpha||^2 \rightarrow \alpha = (B^TB)^{-1}B^Ty$$
where as mentioned $\alpha$ is the vector of $\alpha_j$'s, and each $B_{kl} = h_l(x_k)$

### b

$$S(x) = \bigg\{
\begin{array}{cc}
S_0(x) & \text{if } x < \xi_1 \\
S_1(x) & \text{if } x \in [\xi_1,\xi_2) \Rightarrow \\
S_2(x) & \text{if } x \ge \xi_2,
\end{array}$$

$$S(x) = \bigg\{
\begin{array}{cc}
a_1 + b_1x + c_1 x^2 + d_1 x^3 & \text{if } x < 15 \\
a_2 + b_2 x + c_2 x^2 + d_2 x^3  & \text{if } x \in [15,20) \\
a_3 + b_3 x + c_3 x^2 + d_3 x^3  & \text{if } x \ge 20,
\end{array}$$

with the constraints of: $S_0(15) = S_1(15)\text{, }S_0'(15) = S_1'(15)\text{, }S_0''(15) = S_1''(15)$ and $S_1(15) = S_2(15)\text{, }S_1'(15) = S_2'(15)\text{, }S_1''(15) = S_2''(15)$, and let $v = (a_1, b_1, c_1, d_1, a_2, \dots, d_3)$.

So we have 6 constraints, and 12 parameters ($v$). 

To satisfy the first constraint, $S_0(15) = S_1(15)$, then $a_1 + b_1 \cdot 15 + c_1 \cdot 15^2 + d_1 \cdot 15^3 = a_2 + b_2 \cdot 15 + c_2 \cdot 15^2 + d_2 \cdot 15^3 \rightarrow a_1 + b_1 \cdot 15 + c_1 \cdot 15^2 + d_1 \cdot 15^3 - (a_2 + b_2 \cdot 15 + c_2 \cdot 15^2 + d_2 \cdot 15^3) = 0$

This can also be expressed as $v \cdot w^{(1)} = 0$, where $w^{(1)} = (1,15,15^2,15^3, -1,-15,-15^2,-15^3,0,0,0,0)$.
Similarly for each contraint, we can express each as a $v \cdot w^{(i)} =0$ for $i = 1,2,3,4,5,6$.

For example, $S_1'(20)=S_2'(20) \rightarrow v \cdot w^{(5)} = 0$, where $w^{(5)} = (0,0,0,0,0,1,2 \cdot15, 3\cdot15^2, 0, -1, -2\cdot 15, -3 \cdot 15^2$

Now let $A$ be a $6\times 12$ matrix where each row  $i$is $w^{(i)}$. Then $v$ encodes a cubic spline with knots at 15 and 20 satisfying the above conditions iff $Av=0$, that is $v \in \text{null}(A) \rightarrow \dim(v) = \dim(\text{null}(A))$. 

Suppose $v^{(1)},v^{(2)}, \dots,v^{(D)}$ form a basis for $\text{null}(A)$, then each $v^{(i)}$ encodes a spline $h_{(i)}(x)$. Since $S(x)$ above is a spline in $\mathcal{F}$, then it is encoded by a $v \in \text{null}(A)$.
Since the $v^{(j)}$ form a basis for $\text{null}(A)$, we have $v = \sum_{=1}^D \alpha_jv^{(j)}$.

But we've already shown that $S(x) = \sum_{i=1}^{12}\alpha_ih_i(x)$, therefore our $h_i(x)$ also form a basis for $\text{null}(A)$. Since $A$ is a $6\times12$ matrix, it's nullspace has dimension $12-6=6$, and therefore our spline space also has dimension 6.


### c

#### i

From above, we easily see that $h_1(x)$ is simply a spline with $a_1 = a_2 = a_3 = 1$ and $b_1,b_2,\dots d_3 = 0$.
$h_2(x)$ is a spline with parameters $b_i = 1$, and $a_i=c_i=d_i=0$, $i=1,2,3$, and similarly for $h_3(x)$ and $h_4(x)$. We easily see that $h_i(x)$'s are cubic polynomials over $(-\infty,\zeta_1], (\zeta_1,\zeta_2], (\zeta_2,\infty)$, $i=1,2,3,4$

Now $h_5(x) = [x-\zeta_1]_+^3$. For $h_5(x)$ to be a cubic spline with knots at 15 and 20, it must be a polynomial of at most degree 3, and it must satisfy the three continuity conditions at $x=15,20$. 

$h_5(x) = 0$ if $x \leq 15$, and $h_5(x) = (x-15)^3$ if $x > 15$. So $x$ is clearly a polynomial of degree 3 when $x>15$, and a constant when $x\leq 15$, so our first condition is satisfied. Now we need to show the continuity conditions hold at 15 and 20. At $x=20$, since $h$ is a polynomial of degree 3, it is clear that the continuities hold for $h(x)$ and the first two derivatives. 

At $x=15$, $h(x)=h'(x)=h''(x)=0$. Also:

$\lim_{x\rightarrow 15^+}h(x) = \lim_{x\rightarrow 15^+}(x-15)^3 = 0$

$\lim_{x\rightarrow 15^+}h'(x) = \lim_{x\rightarrow 15^+}3(x-15)^2 = 0$

$\lim_{x\rightarrow 15^+}h''(x) = \lim_{x\rightarrow 15^+}6(x-15) = 0$

Thus, our continuity conditions hold at $x=15$, so $h_5(x)$ fulfills all the conditions to be a cubic spline with knots at $\zeta = (15,20)$.

Similarly, $h_6(x) = 0$ at $x=15$, so all ocnditions are easily met there. At $x=20$, we have $h_6(x) = h_6'(x) = h_6''(x) = 0$, and the limits from the RHS:


$\lim_{x\rightarrow 20^+}h_6(x) = \lim_{x\rightarrow 20^+}(x-20)^3 = 0$

$\lim_{x\rightarrow 20^+}h_6'(x) = \lim_{x\rightarrow 20^+}3(x-20)^2 = 0$

$\lim_{x\rightarrow 20^+}h_6''(x) = \lim_{x\rightarrow 20^+}6(x-20) = 0$

So all of our continuity conditions are met, and thus $h_6(x)$ is a cubic spline satidfying conditions for $\zeta = (15,20)$

#### ii

Suppose $\exists \alpha$ such that $\sum_{i=1}^6\alpha_i h_i(x) = 0 \forall x$

Then we can pick any x and get $\alpha_1h_1(x) + \alpha_2 h_2(x) + \alpha_3 h_3(x) + \alpha_4 h_4(x) + \alpha_5 h_5(x) + \alpha_6 h_6(x) = 0$. Ie, let $x=5$, then we get $\alpha_1h_1(5) + \alpha_2 h_2(5) + \alpha_3 h_3(5) + \alpha_4 h_4(5) + \alpha_5 h_5(5) + \alpha_6 h_6(5) = \alpha_1 + 5 \alpha_2 + 25 \alpha_3 + 125 \alpha_4 + 0 \alpha_5 + 0 \alpha_6 = 0$. We can repeat this process 5 more times, giving us 6 constraints on our $\alpha$. Make a matrix $A$, with 6 rows, where each row is coefficients of $\alpha$ for an as exampled above. If we can make a linear combination $h_i(x)$, then we would get $A \alpha = 0$. However, if $A$ is invertible, then we get $A^{-1}A\alpha = 0 \rightarrow \alpha = A^{-1}0\rightarrow \alpha = 0$. This would be a contradiction, as we said that $h_i(x)$'s formed a non-arbitray linear combination equal to 0, thus they are linearly independent. 

Note: we must x values greater than 15 and greater than 20, otherwise we will not have enough constraints and we will be able to have linear combinations equal to 0 over a limited domain of $x$'s

```{r results = 'asis'}
A = matrix(0, nrow = 6, ncol = 6)

x_tests <- c(10, 14, 16, 19,21, 25)

for (i in 1:6) {
  
  x <- x_tests[i]
  A[i,] <- c(1, x, x^2, x^3,
             ifelse(x < 15, 0, (x-15)^3),
             ifelse(x < 20, 0, (x-20)^3)
  )
}
rownames(A) <- paste("x", x_tests, sep="=")
print(xtable::xtable(A), comment = F)

det(A)
```

Since the $\det(A) \neq 0$, then A is an invertible matrix thus $\alpha = 0$, and we do not have a linear combination of $h_i(x)=0$, so $h_i(x)$ are linearly independent.

Code + results to show why we need to include x values on both sides of the knots:

```{r results = 'asis'}
A = matrix(0, nrow = 6, ncol = 6)

x_tests <- c(1,5,2,8,10,14)

for (i in 1:6) {
  
  x <- x_tests[i]
  A[i,] <- c(1, x, x^2, x^3,
             ifelse(x < 15, 0, (x-15)^3),
             ifelse(x < 20, 0, (x-20)^3)
  )
}
rownames(A) <- paste("x", x_tests, sep="=")
print(xtable::xtable(A), comment = F)

det(A)
```

If we dont have x values on both sides of the knots, then we can will get a result such that $\det(A) = 0$, and be falsely led to believe we had linearly dependent $h_i(x)$, but that is because we did not have constraints for all 6 functions.

#### iii) Show each spline is a linear combination the 6 functions

Since in **i** we showed that the dimension of $\mathcal{F} = 6$, and in **ii** we showed that the 6 $h_i(x)$'s are linearly independent, then $h_i(x)$'s form a basis for $\mathcal{F}$. Since the spline $S(x) \in \mathcal{F}$, $S(x) = \alpha_1 h_1(x) + \alpha_2 h_2(x) + \alpha_3 h_3(x) + \alpha_4 h_4(x) + \alpha_5 h_5(x) + \alpha_6 h_6(x)$ for some $\alpha$, that is $S(x)$ is a linear combination of $h_1(x), h_2(x), h_3(x), h_4(x), h_5(x), h_6(x)$.


### d
```{r results='asis', message=FALSE}
bone_mass <- read_delim("BoneMassData.txt", delim = " ")

females <- bone_mass %>% filter(gender == "female")


y <- females$spnbmd
x <- females$age

h_5 <- function(x, zeta_1 = 15){ifelse((x - zeta_1) > 0, (x - zeta_1)^3, 0)}
h_6 <- function(x, zeta_2 = 20){ifelse((x - zeta_2) > 0, (x - zeta_2)^3, 0)}

B <- matrix(
  c(rep(1, length(x)),
  x,
  x^2,
  x^3,
  h_5(x),
  h_6(x)),
  ncol = 6
)

alpha <- solve(t(B) %*% B) %*% t(B) %*% y

```

So $S(x) =$`r alpha[1]` + `r alpha[2]`$x$ + `r alpha[3]`$x^2$ + `r alpha[4]`$x^3$ + `r alpha[5]`$[x-15]_+^3$ + `r alpha[6]`$[x-20]_+^3$


```{r results = 'asis'}

S_x <- function(x, zeta_1 = 15, zeta_2 = 20, A = alpha){
  H <- matrix(
    c(rep(1, length(x)),
    x, 
    x^2, 
    x^3, 
    h_5(x), 
    h_6(x)),
    ncol = 6
  )
  Sx <- apply(H,1,function(x){sum(A*x)})
  return(Sx)
}

ggplot(data = females, aes(x = age, y = spnbmd)) +
  geom_point() +
  theme_bw() +
  stat_function(fun = S_x, color = "#1B9E77", size = 1.5) +
  geom_smooth(method = "lm", formula = y ~ bs(x, knots=c(15,20)), 
              alpha = 0, color = "#D95F02", linetype = 2, size = 1) +
  labs(x = "Age", y = "Bone Mass Density", 
       title = "Bone Mass Density by Age", 
       subtitle = "Cubic Spline Regression with knots at 15, 20")

```
Here the solid teal line is our $S(x)$ plotted over the data, and the dashed orange line is the spline regression formed using the `bs` function to compute the spline with knots at 15 and 20. As you can see, the regressions match up perfectly.

## 3

$f(x) = e^x \rightarrow f'(x) = e^x \rightarrow f'(0)=1$

```{r results='asis'}
options(digits = 16)
i <- seq(-20,0,1)
h <- 10^i

finite_difference_1 <- function(x,h){
  fx <- (exp(x + h) - exp(x)) / h
  
  return(fx)
}

finite_difference_2 <- function(x,h){
  fx <- (exp(x + h) - exp(x - h)) / (2 * h)
  
  return(fx)
}

differences_1 <- sapply(h, finite_difference_1, x = 0)
differences_2 <- sapply(h, finite_difference_2, x = 0)

results <- data.frame(
  h = format(10^i, scientific = T, digits = 2),
  fin_diff_1 = differences_1,
  fin_diff_2 = differences_2
)
```
```{r echo = FALSE,  results='asis'}
print(xtable::xtable(results, digits = 16), comment = F)
```
```{r  results='asis'}
result_errors <- data.frame(h = results$h,
                             error_1 = results$fin_diff_1 - 1,
                             error_2 = results$fin_diff_2 - 1)
```
```{r echo = FALSE,  results='asis'}

print(xtable::xtable(result_errors, digits = 16), comment = F)
```

So we see that our highest accuracy was achieved at `h=1e-05` using the second finite difference method, where we were accurate out to 10 digits. Using the first finite difference method, we were able to achieve eight digits of accuracy at `h=1e-08`.

**Reasons for error:**

Magnitude of error is given by $|F_h(x) - f(x)|$. While the numerical formula for $F_h(x) = \frac{f(x+h)-f(x)}{h}$, because of computing restrictions the actual formula we are computing is $$F_h^{(1)}(x) = \frac{f(x+h)(1+\epsilon_1)-f(x)(1+\epsilon_2)}{h} = 
\\
\frac{f(x+h)-f(x)}{h} + \frac{f(x+h)\epsilon_1-f(x)\epsilon_2}{h} \text{,  } |\epsilon_i|<10^{-16}$$

Here the first term is our Taylor Series error, and the second term is our round off error due to machine limitations.

Now we can re-write 
$$F_h(x) = \frac{[f(x)+f'(x)h+\frac{1}{2}f''(x)h^2]-f(x)}{h} + \frac{C\epsilon_m}{h} = f'(x)+\frac{1}{2}f''(x)h +\frac{C\epsilon_m}{h}$$
where $\epsilon_m=10^{-16}$, ie machine epsilon.

So our error becomes $|F_h(x)-f'(x)| = |\frac{1}{2}f''(x)h| + \frac{C\epsilon_m}{h}$, take the derivative with respect to $h$ and set equal to 0 to solve for the optimal h to minimize error: 
$$\left|\frac{1}{2}f''(x)\right| -\frac{C\epsilon_m}{h^2}=0 \rightarrow h^2 = \frac{C\epsilon_m}{|1/2f''(x)|} \rightarrow h \approx \sqrt{\epsilon_m} = 10^-8$$
And our error comes out to $\text{O}10^{-8}$

So this explains why `h=1e-08` produced the smallest error for our first finite difference equation. To explain some of the output a little further: the reason our output is exactly 0 for `h=1e-16` and smaller is because when we subtract that from $f(x)=1$, the computer is only able to store 16 significant figures, so the extra $1\text{e}-16$ can't be stored, and the computer is actually calculating $\frac{1-1}{10^{-16}} = 0$.

Looking at our second finite difference equation, we follow the same process but see we are able to reach slightly more accurate results, and with a larger $h$.

\begin{equation}
\begin{split}
\tilde{F}_h(x) = \frac{f(x+h)-f(x-h)}{2h} = \frac{f(x+h)(1+\epsilon_1)-f(x-h)(1+\epsilon_2)}{2h} = \\ \frac{f(x+h)-f(x-h)}{2h} + \frac{f(x+h)\epsilon_1-f(x-h)\epsilon_2}{2h} 
\end{split}
\end{equation}

Using Taylor Series expansion, the difference in the numerator of the first term becomes:

$f(x) + f'(x)h + \frac{1}{2}f''(x)h^2 + \frac{1}{6}h^3 - (f(x) - f'(x)h + \frac{1}{2}f''(x)h^2 - \frac{1}{6}h^3) = 2hf'(x) + 1/3f'''(x_h^3)$

and this gives:

$\tilde{F}_h(x) = f'(x)+\frac{1}{6}f'''(x)h^2+\frac{C\epsilon_m}{h}$

$|\tilde{F}_h(x) - f'(x)| = \left|\frac{1}{6}f'''(x)h^2+\frac{C\epsilon_m}{h}\right|$

Take the derivative and set equal to 0: $\frac{2}{6}f'''(x)h - \frac{C\epsilon_m}{h^2}=0 \rightarrow h^3 = \frac{3C\epsilon_m}{f'''(x)}\approx (\epsilon_m)^{1/3} = 10^{-16/3}$, which means our error comes out to $\text{O}10^{-32/3}$, which is nearly 100 times more accurate than the first equation.


## 4

In general, the cdf $F(x)$ of a distribution is $P(X\leq x) \rightarrow P(-\infty < X \leq x)$. However here we have defined $F(X)$ with a lower bound of 0. Since we know the standard normal distribution is centered at 0, with $P(X\leq 0) = 0.5$ and $P(X>0) = 0.5$, then
$$F(x) = \int_{0}^x dz \frac{1}{\sqrt{2\pi}} \text{e}^{-z^2/2} \rightarrow F(\infty) = 0.5$$
Using `R`'s built in integrate function, we see that an upper limit of 7.5 gives a value of 0.4999999999999883, accurate to 12 decimal places, so we set the upper limit for our subdivisions at 7.5. Increasing the upperbound beyond this will give us minimal added area and increase our error. However, we make this bound adjustable to the function's user.

```{r results = 'asis'}
norm_cdf <- function(z){1 / (sqrt(2 * pi)) * exp(-z^2 / 2)}
integrate(norm_cdf, 0, 7.5, subdivisions = 100)

Fapprox <- function(n, method = "reimann", upperlimit = 7.5){
  
  h <- upperlimit / n
  
  rectangle <- function(z){
    height <- 1 / sqrt(2 * pi) * exp(-(z^2) / 2)
    width <- h
    area <- height * width
    return(area)
  }
  
  trapezoid <- function(z){
    height_1 <- 1 / sqrt(2 * pi) * exp(-(z^2) / 2)
    height_2 <- 1 / sqrt(2 * pi) * exp(-((z + h)^2) / 2)
    area <- (height_1 + height_2) / 2 * h
    
  }
  
  if(method == "reimann"){
    cuts <- seq(0, upperlimit, h)
    norm_cdf <- sum(sapply(cuts[-n], rectangle))
  }else if(method == "trapezoid"){
    cuts <- seq(0, upperlimit, h)
    norm_cdf <- sum(sapply(cuts[-n], trapezoid))
  }else if(method == "useR"){
    norm_cdf_func <- function(z){1 / (sqrt(2 * pi)) * exp(-z^2 / 2)}
    norm_cdf <- integrate(norm_cdf_func, 0, 
                          upperlimit, 
                          subdivisions = n)$value
  }else{
    return("Please enter a valid integration method.")
  }
  return(norm_cdf)
  
}

n_s <- c(10,100,1000,10000)
evaluations <- data.frame(
  n = format(n_s, digits = 0),
  Riemann = sapply(n_s, Fapprox),
  Trapezoid = sapply(n_s, Fapprox, method = "trapezoid"),
  useR = sapply(n_s, Fapprox, method = "useR")
)

print(xtable::xtable(evaluations, digits = 16), comment = F)


```

Looking at the table of our results, we see that the `rectangle` method consistently overestimates the probability by a good amount, only reaching 3 digits of accuracy after we bump `n` to 10000. This is what we would expect as the normal curve is a decreasing function so we would consistently be adding additional area at each step. The trapezoid method is much more accurate, achieving 10 digits of accuracy at nust `n=10`, and almost reaching the same accuracy as `R` at `n=10000`. Since the normal distribution is concave within the first standard deviation ($|z| < 1$), the trapezoid method will underestimate the area. But the normal distribution is convex beyond the first standard deviation ($|z| > 1$), so the trapezoid method will overestimate the area in the tail. The over/under estimations largely cancel out in our calculation, but since there is more area within the first standard deviation we still end up slightly underestimating the cdf. The accuracy of `R`'s built in integral function did not change with varying values of `n`.

